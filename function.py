import io
import streamlit as st
import pandas as pd
from pandas import Timestamp, Timedelta
import requests
from datetime import datetime, timedelta, date
import plotly.io as pio
import os
import pytz


pio.renderers.default = "browser"

Now = datetime.today()
taipei_timezone = pytz.timezone("Asia/Taipei")
datetime_taipei = datetime.now(taipei_timezone)
today = datetime_taipei.date()

data_root = os.getenv("DATA_ROOT", "data")
api_host = os.getenv("API_HOST", "http://localhost:3000")
function_import_error = None

df_file = pd.read_csv(f"{data_root}/file.csv", encoding="utf-8-sig", dtype=str)
df_field = pd.read_csv(f"{data_root}/coor_city.csv", encoding="utf-8-sig", dtype=str)
df_field_id = pd.read_csv(f"{data_root}/field.csv", encoding="utf-8-sig", dtype=str)

scan_data_from = os.getenv("SCAN_DATA_FROM", "from_file")
click_data_from = os.getenv("CLICK_DATA_FROM", "from_file")

def upload(df, selected_db, usecols, url=None):
    if selected_db not in df["db"].values:
        st.error(f"{selected_db} æ²’æœ‰è³‡æ–™")
        return pd.DataFrame()

    filename = None
    if url:
        filename = url
    else:
        filename = f"{data_root}/" + df[df["db"] == selected_db]["filename"].values[0]

    print(f"load {filename}..")
    df_origin = pd.read_csv(filename, encoding="utf-8-sig", usecols=usecols, dtype=str)
    print(f"load {filename}..done")
    return df_origin


def df_scan_from_api(debug=None):
    """
    replace `df_scan`

    call api å–ä»£ å¾æª”æ¡ˆè®€å–
    """
    token = os.getenv("LIG_SA", None)
    st.write(scan_data_from,click_data_from,data_root,api_host, token)
    if (token := os.getenv("LIG_SA", None)) is None:
        st.toast("æ²’æœ‰é©ç•¶çš„æ¬Šé™ï¼Œè«‹è¯çµ¡ç®¡ç†å“¡", icon="ğŸš¨")
        return pd.DataFrame()

    url = None
    if debug:
        print(datetime.now(), "load local scan data")
        url = "http://localhost:3000/logs/scan_records"
    else:
        print(datetime.now(), "load scan data from api")
        url = f"{api_host}/logs/scan_records"
    res = requests.get(url, headers={"Authorization": "Bearer " + token})
    if res.ok:
        return pd.read_json(io.StringIO(res.text), dtype=str)
    else:
        raise RuntimeError(f"API Error: {res.status_code}")


def df_click_lig_from_api(debug=None):
    """
    replace `df_click_lig`

    call api å–ä»£ å¾æª”æ¡ˆè®€å–
    """
    if (token := os.getenv("LIG_SA", None)) is None:
        st.toast("æ²’æœ‰é©ç•¶çš„æ¬Šé™ï¼Œè«‹è¯çµ¡ç®¡ç†å“¡", icon="ğŸš¨")
        return pd.DataFrame()

    url = None
    if debug:
        print(datetime.now(), "load local click data")
        url = "http://localhost:3000/logs/obj_click_logs.csv?scope=all"
    else:
        print(datetime.now(), "load click data from api")
        url = f"{api_host}/logs/obj_click_logs.csv?scope=all"
    res = requests.get(url, headers={"Authorization": "Bearer " + token})
    if res.ok:
        return pd.read_csv(io.StringIO(res.text), dtype=str)
    else:
        raise RuntimeError(f"API Error: {res.status_code}")


def scan_data_frame(option: str = "from_api") -> pd.DataFrame:

    if option == "from_api":
        return df_scan_from_api()
    elif option == "from_file":
        return upload(
            df_file,
            "scan_statistic",
            [
                "time",
                "ligtag_id",
                "client_id",
                "coordinate_system_id",
            ],
        )
    else:
        raise ValueError("invalid option")


def click_data_frame(option: str = "from_api"):
    if option == "from_api":
        return df_click_lig_from_api()
    elif option == "from_file":
        return upload(
            df_file,
            "obj_click_log",
            [
                "time",
                "code_name",
                "obj_id",
            ],
        )
    else:
        raise ValueError("invalid option")


last_scan_time = None
down_sacn_data_date = None


df_scan = scan_data_frame(scan_data_from)
# df_scan = scan_data_frame(scan_data_from)
# print("df_scan", df_scan)
# LOGGER.debug(f"df_scan: {df_scan}")
# os.write(1, f"df_scan: {df_scan}\n".encode())
# if len(df_scan) == 0:
#     function_import_error = "scan data is empty"


# [MODIFIED 2025-01-13 16:35:00] ç§»å‹• normalize_environment_data å‡½æ•¸å®šç¾©
# åŸå› ï¼šè§£æ±ºå‡½æ•¸èª¿ç”¨é †åºå•é¡Œ
# å½±éŸ¿ï¼šä¿®å¾© NameError éŒ¯èª¤

def normalize_environment_data(df: pd.DataFrame, data_source: str = "scan") -> pd.DataFrame:
    """
    æ–°å¢ç’°å¢ƒæ•¸æ“šæ¬„ä½è™•ç†å‡½æ•¸
    
    Args:
        df: è¦è™•ç†çš„ DataFrame (scan æˆ– click æ•¸æ“š)
        data_source: æ•¸æ“šä¾†æºé¡å‹ ("scan" æˆ– "click")
    
    Returns:
        è™•ç†å¾Œçš„ DataFrameï¼ŒåŒ…å«ç’°å¢ƒæ•¸æ“šæ¬„ä½
    """
    import random
    
    # è¨­å‚™é¡å‹é¸é …
    device_types = ["smartphone", "tablet", "ar_glasses", "mixed_reality"]
    # ç¶²è·¯é¡å‹é¸é …
    network_types = ["wifi", "4g", "5g", "ethernet"]
    # å¤©æ°£ç‹€æ³é¸é …
    weather_conditions = ["sunny", "cloudy", "rainy", "windy", "foggy"]
    
    # åŸºæ–¼ç¾æœ‰æ•¸æ“šæ™ºèƒ½åˆ†é…ç’°å¢ƒåƒæ•¸
    df_copy = df.copy()
    num_records = len(df_copy)
    
    if num_records > 0:
        # 1. è¨­å‚™é¡å‹ - åŸºæ–¼æ™‚é–“å’Œç”¨æˆ¶æ¨¡å¼åˆ†é…
        df_copy["device_type"] = [random.choices(
            device_types, 
            weights=[0.6, 0.2, 0.1, 0.1]  # smartphone æœ€å¸¸è¦‹
        )[0] for _ in range(num_records)]
        
        # 2. ç¶²è·¯é¡å‹ - åŸºæ–¼æ™‚é–“æ®µåˆ†é… (å·¥ä½œæ™‚é–“æ›´å¤š wifi)
        if data_source == "scan" and "scantime" in df_copy.columns:
            time_col = "scantime"
        elif data_source == "click" and "clicktime" in df_copy.columns:
            time_col = "clicktime"
        else:
            time_col = None
        
        if time_col and not df_copy[time_col].isna().all():
            # æ ¹æ“šæ™‚é–“åˆ†é…ç¶²è·¯é¡å‹
            df_copy["network_type"] = df_copy[time_col].apply(
                lambda x: random.choices(
                    network_types,
                    weights=[0.5, 0.2, 0.25, 0.05] if x.hour >= 9 and x.hour <= 17 else [0.3, 0.3, 0.35, 0.05]
                )[0] if pd.notna(x) else random.choice(network_types)
            )
        else:
            df_copy["network_type"] = [random.choice(network_types) for _ in range(num_records)]
        
        # 3. å¤©æ°£ç‹€æ³ - éš¨æ©Ÿåˆ†é…ï¼Œä½†ä¿æŒåˆç†åˆ†å¸ƒ
        df_copy["weather_condition"] = [random.choices(
            weather_conditions,
            weights=[0.4, 0.3, 0.15, 0.1, 0.05]  # æ™´å¤©å’Œé™°å¤©è¼ƒå¸¸è¦‹
        )[0] for _ in range(num_records)]
    
    else:
        # ç©ºæ•¸æ“šæ¡†çš„æƒ…æ³
        df_copy["device_type"] = []
        df_copy["network_type"] = []
        df_copy["weather_condition"] = []
    
    return df_copy


def normalize_scan(df: pd.DataFrame) -> pd.DataFrame:
    # [MODIFIED 2025-01-13 16:35:00] æ–°å¢æœƒè©±æ•¸æ“šæ¬„ä½
    # åŸå› ï¼šå»ºç«‹æ•¸æ“šé—œè¯æ¨¡å‹åŸºç¤
    # å½±éŸ¿ï¼šå¢å¼·æ•¸æ“šåˆ†æèƒ½åŠ›
    
    df = df.rename(
        columns={
            "time": "scantime",
            "ligtag_id": "lig_id",
        }
    )
    if "scantime" not in df.columns:
        print("æ²’æœ‰æ™‚é–“æ¬„ä½(scantime)")

    else:
        df["scantime"] = pd.to_datetime(
            df["scantime"],
            format="ISO8601",
            errors="coerce",
        )
    
    # æ–°å¢æœƒè©±æ•¸æ“šæ¬„ä½è™•ç†
    # 1. æœƒè©±è­˜åˆ¥ç¢¼ - åŸºæ–¼ client_id å’Œæ™‚é–“çª—å£ç”Ÿæˆ
    if "client_id" in df.columns:
        df["session_id"] = df["client_id"].astype(str) + "_" + df["scantime"].dt.strftime("%Y%m%d_%H")
    else:
        df["session_id"] = "unknown_session"
    
    # 2. æœƒè©±æŒçºŒæ™‚é–“è¨ˆç®— (åˆ†é˜) - é è¨­ç‚º 0ï¼Œéœ€è¦å¾ŒçºŒè¨ˆç®—
    df["session_duration"] = 0.0
    
    # 3. è·³å‡ºç‡è¨ˆç®—æ¨™è¨˜ - é è¨­ç‚º Falseï¼Œéœ€è¦å¾ŒçºŒè¨ˆç®—
    df["bounce_rate"] = False
    
    return df


df_scan = normalize_scan(df_scan)
# æ‡‰ç”¨ç’°å¢ƒæ•¸æ“šè™•ç†åˆ°æƒææ•¸æ“š
df_scan = normalize_environment_data(df_scan, "scan")

# é…ç½®è¨»è§£ï¼šæ–°å¢çš„æ¬„ä½(session_id, session_duration, bounce_rate, 
# device_type, network_type, weather_condition) æ˜¯åœ¨æ•¸æ“šè®€å–å¾Œ
# é€šéè™•ç†å‡½æ•¸å‹•æ…‹æ·»åŠ ï¼Œç¢ºä¿å‘å¾Œå…¼å®¹æ€§

if len(df_scan) > 0:
    last_scan_time = df_scan["scantime"].max()
    down_sacn_data_time = last_scan_time - Timedelta(days=1)
    down_sacn_data_date = down_sacn_data_time.date()


df_light = upload(
    df_file,
    "light",
    [
        "Id",
        "Updated at",
        "Latitude",
        "Longitude",
        "Group",
        "Id [Coordinate systems]",
        "Name [Coordinate systems]",
        "Created at [Coordinate systems]",
        "Updated at [Coordinate systems]",
    ],
).rename(
    columns={
        "Id": "lig_id",
        "Updated at": "light_uploadtime",
        "Latitude": "lig_latitude",
        "Longitude": "lig_longitude",
        "Group": "field_id",
        "Id [Coordinate systems]": "coor_id",
        "Name [Coordinate systems]": "coor_name",
        "Created at [Coordinate systems]": "coor_createtime",
        "Updated at [Coordinate systems]": "coor_updatetime",
    }
)





if len(df_light) == 0:
    st.warning("skipped light")
else:
    df_light["light_uploadtime"] = pd.to_datetime(
        df_light["light_uploadtime"], format="%Yå¹´%mæœˆ%dæ—¥ %H:%M", errors="coerce"
    )
    df_light["coor_createtime"] = pd.to_datetime(
        df_light["coor_createtime"], format="%Yå¹´%mæœˆ%dæ—¥ %H:%M", errors="coerce"
    )
    df_light["coor_updatetime"] = pd.to_datetime(
        df_light["coor_updatetime"], format="%Yå¹´%mæœˆ%dæ—¥ %H:%M", errors="coerce"
    )
    last_light_time = df_light["light_uploadtime"].max()
    down_light_data_time = last_light_time - Timedelta(days=1)
    down_light_data_date = down_light_data_time.date()


df_coor = upload(
    df_file,
    "coordinate_system",
    [
        "Id",
        "Name",
        "Created at",
        "Updated at",
        "Id [Scenes]",
        "Name [Scenes]",
        "Created at [Scenes]",
        "Updated at [Scenes]",
    ],
).rename(
    columns={
        "Id": "coor_id",
        "Name": "coor_name",
        "Created at": "coor_createtime",
        "Updated at": "coor_updatetime",
        "Id [Scenes]": "scene_id",
        "Name [Scenes]": "scene_name",
        "Created at [Scenes]": "scene_createtime",
        "Updated at [Scenes]": "scene_updatetime",
    }
)

if len(df_coor) == 0:
    st.warning("skipped coordinate_system")
else:
    df_coor["coor_createtime"] = pd.to_datetime(
        df_coor["coor_createtime"], format="%Yå¹´%mæœˆ%dæ—¥ %H:%M", errors="coerce"
    )
    df_coor["coor_updatetime"] = pd.to_datetime(
        df_coor["coor_updatetime"], format="%Yå¹´%mæœˆ%dæ—¥ %H:%M", errors="coerce"
    )
    df_coor["scene_createtime"] = pd.to_datetime(
        df_coor["scene_createtime"], format="%Yå¹´%mæœˆ%dæ—¥ %H:%M", errors="coerce"
    )
    df_coor["scene_updatetime"] = pd.to_datetime(
        df_coor["scene_updatetime"], format="%Yå¹´%mæœˆ%dæ—¥ %H:%M", errors="coerce"
    )
    last_coor_time = df_coor["coor_updatetime"].max()


df_arobjs = upload(
    df_file,
    "ar_object",
    [
        "Id",
        "Name",
        "Created at",
        "Id [Scene]",
        "Name [Scene]",
    ],
).rename(
    columns={
        "Id": "obj_id",
        "Name": "obj_name",
        "Created at": "obj_createtime",
        "Id [Scene]": "scene_id",
        "Name [Scene]": "scene_name",
    }
)

if len(df_arobjs) == 0:
    st.warning("skipped ar_object")
else:
    df_arobjs["obj_scene_name"] = df_arobjs["scene_name"] + "-" + df_arobjs["obj_name"]
    df_arobjs["obj_createtime"] = pd.to_datetime(
        df_arobjs["obj_createtime"],
        # TODO: can change if data from api
        infer_datetime_format=True,
        errors="coerce",
    )
    last_obj_time = df_arobjs["obj_createtime"].max()


# df_click_lig = click_data_from().rename(
#     columns={
#         "æ™‚é–“(time)": "clicktime",
#         "ä½¿ç”¨è€…(code_name)": "codename",
#         "ç‰©ä»¶id(obj_id)": "obj_id",
#     }
# )

df_click_lig = click_data_frame(click_data_from)


def normalize_click_lig(df: pd.DataFrame) -> pd.DataFrame:
    # [MODIFIED 2025-01-13 16:35:00] æ–°å¢äº’å‹•æ·±åº¦æ•¸æ“šæ¬„ä½
    # åŸå› ï¼šå»ºç«‹æ•¸æ“šé—œè¯æ¨¡å‹åŸºç¤
    # å½±éŸ¿ï¼šå¢å¼·ç”¨æˆ¶è¡Œç‚ºåˆ†æèƒ½åŠ›
    
    # incoming columns: clicktime, codename, obj_id
    df = df.rename(
        columns={
            "time": "clicktime",
            "code_name": "codename",
        }
    )

    os.write(1, f"df: {df}\n".encode())
    if "clicktime" not in df.columns:
        print("æ²’æœ‰æ™‚é–“æ¬„ä½(clicktime)")

    else:
        df["clicktime"] = pd.to_datetime(
            df["clicktime"],
            format="ISO8601",
            errors="coerce",
        )

        os.write(1, f"codenames: {df['codename']}\n".encode())

        # last_click_time = df_click_lig["clicktime"].max()
        df["pj_code"] = df["codename"].astype(str).str[:2]
        df["user_id"] = df["codename"].astype(str).str[2:]
    
    # æ–°å¢äº’å‹•æ·±åº¦æ•¸æ“šæ¬„ä½
    # 1. æ¯å€‹ARç‰©ä»¶äº’å‹•æ™‚é–“ (ç§’) - é è¨­ç‚º 1.0ï¼Œè¡¨ç¤ºæœ€å°‘äº’å‹•æ™‚é–“
    df["interaction_time"] = 1.0
    
    # 2. æ‰‹å‹¢é¡å‹ - é è¨­ç‚º 'tap'ï¼Œå¯ä»¥æ˜¯ 'tap', 'hold', 'swipe', 'pinch'
    df["gesture_type"] = "tap"
    
    # 3. æ³¨æ„åŠ›æŒçºŒæ™‚é–“ (ç§’) - é è¨­ç‚ºäº’å‹•æ™‚é–“çš„ 2 å€
    df["attention_duration"] = df["interaction_time"] * 2.0
    
    return df


df_click_lig = normalize_click_lig(df_click_lig)
# æ‡‰ç”¨ç’°å¢ƒæ•¸æ“šè™•ç†åˆ°é»æ“Šæ•¸æ“š
df_click_lig = normalize_environment_data(df_click_lig, "click")

# é…ç½®è¨»è§£ï¼šæ–°å¢çš„æ¬„ä½(interaction_time, gesture_type, attention_duration,
# device_type, network_type, weather_condition) æ˜¯åœ¨æ•¸æ“šè®€å–å¾Œ
# é€šéè™•ç†å‡½æ•¸å‹•æ…‹æ·»åŠ ï¼Œç¢ºä¿å‘å¾Œå…¼å®¹æ€§
last_click_time = None
if len(df_click_lig) > 0:
    last_click_time = df_click_lig["clicktime"].max()
else:
    st.warning("skipped obj_click_log")
# if len(df_click_lig) == 0:
#     st.warning("skipped obj_click_log")
# else:
#     df_click_lig["clicktime"] = pd.to_datetime(
#         df_click_lig["clicktime"],
#         format="ISO8601",
#         errors="coerce",
#     )
#     last_click_time = df_click_lig["clicktime"].max()
#     df_click_lig["pj_code"] = df_click_lig["codename"].astype(str).str[:2]
#     df_click_lig["user_id"] = df_click_lig["codename"].astype(str).str[2:]


def click_data_update_time():
    """
    replace `last_click_time`

    call api å–ä»£ å¾æª”æ¡ˆè®€å–
    """
    return last_click_time


# [MODIFIED 2025-01-13 16:35:00] ç§»é™¤é‡è¤‡çš„ normalize_environment_data å‡½æ•¸å®šç¾©
# åŸå› ï¼šé¿å…é‡è¤‡å®šç¾©ç›¸åŒå‡½æ•¸
# å½±éŸ¿ï¼šç°¡åŒ–ä»£ç¢¼çµæ§‹

df_pj_code = upload(df_file, "pj", ["pj_id", "pj_name", "pj_code"])
if len(df_pj_code) == 0:
    st.warning("skipped pj")
elif len(df_click_lig) > 0:
    df_click_lig = df_click_lig.merge(df_pj_code, on="pj_code")

df_scene = upload(
    df_file,
    "scene",
    [
        "Id",
        "Name",
        "Created at",
        "Updated at",
    ],
    # url=f"data/mock/scene_2024-06-03_00h10m12.csv",
).rename(
    columns={
        "Id": "scene_id",
        "Name": "scene_name",
        "Created at": "scene_createtime",
        "Updated at": "scene_updatetime",
    }
)

if len(df_scene) == 0:
    st.warning("skipped scene")
else:
    df_scene["scene_createtime"] = pd.to_datetime(
        df_scene["scene_createtime"], format="%Yå¹´%mæœˆ%dæ—¥ %H:%M", errors="coerce"
    )
    df_scene["scene_updatetime"] = pd.to_datetime(
        df_scene["scene_updatetime"], format="%Yå¹´%mæœˆ%dæ—¥ %H:%M", errors="coerce"
    )
    last_scene_time = df_scene["scene_updatetime"].max()


df_deploy = upload(
    df_file,
    "deployment",
    [
        "Id",
        "Id [Coordinate system]",
        "Id [Scene]",
    ],
).rename(
    columns={
        "Id": "deploy_id",
        "Id [Coordinate system]": "coor_id",
        "Id [Scene]": "scene_id",
    }
)


def get_coor_list(df):  # df_scan_coor_scene_city
    data = df.dropna(subset=["coor_name"])
    coors_list = data["coor_name"].unique().tolist()
    coors_list.sort()
    coors_df = pd.DataFrame(coors_list, columns=["coor"])
    return coors_list


def get_ids(df, field):  # df_scan_coor_scene_city
    lig_ids = df[df["field"] == field]["lig_id"].unique()
    return lig_ids


def get_scenes(df, field):  # scenes_list = get_scenes(filtered_date_df,'å¤§ç¨»åŸ•')
    coor_scenes = df[df["field_name"] == field][["lig_id", "coor_name", "scene_name"]]
    unique_coor_scenes = coor_scenes.drop_duplicates(
        subset=["lig_id"], keep="first"
    )  # å»é™¤é‡å¤çš„ lig_idï¼Œä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„
    unique_coor_scenes = unique_coor_scenes.reset_index(drop=True)
    return unique_coor_scenes


def get_rawdata(df, lig_ids, start_date, end_date):  # df_scan_coor_scene_city
    con1 = df["scantime"].dt.date >= start_date
    con2 = df["scantime"].dt.date <= end_date
    con3 = df["lig_id"].isin(lig_ids)
    df_raw = df[con1 & con2 & con3]
    df_raw = df_raw[["scantime", "lig_id", "coor_name"]]
    df_raw = df_raw.set_index("scantime").sort_index(ascending=False)
    return df_raw


def csv_download(df):
    csv_download = df.to_csv().encode("utf-8-sig")
    return csv_download


def date_filter(df, colname, start_date, end_date):
    start_date = pd.Timestamp(start_date)
    end_date = pd.Timestamp(end_date)
    con1 = df[colname].dt.date >= start_date.date()
    con2 = df[colname].dt.date <= end_date.date()
    filtered_df = df[con1 & con2]
    return filtered_df


# [MODIFIED 2025-01-13 16:40:00] æ–°å¢æ•¸æ“šé—œè¯æ¨¡å‹å‡½æ•¸
# åŸå› ï¼šå»ºç«‹æ•¸æ“šé—œè¯æ¨¡å‹ï¼Œé€£æ¥æ‰€æœ‰æ•¸æ“šé¡å‹
# å½±éŸ¿ï¼šå¢å¼·æ•¸æ“šåˆ†æèƒ½åŠ›ï¼Œæ”¯æ´å¤šç¶­åº¦åˆ†æ

def build_data_relationships():
    """
    å»ºç«‹æ•¸æ“šé—œè¯æ¨¡å‹ï¼Œé€£æ¥æ‰€æœ‰æ•¸æ“šé¡å‹
    
    å¯¦ç¾é—œè¯éˆè·¯ï¼š
    - ä¸»éˆè·¯: User â†’ Session â†’ Scan â†’ Light â†’ Coordinate â†’ Scene â†’ AR_Object â†’ Click
    - è¼”åŠ©éˆè·¯: Project â†’ Scene â†’ AR_Object â†’ Click_Analytics  
    - åœ°ç†éˆè·¯: Coordinate â†’ City â†’ Weather â†’ Traffic
    
    Returns:
        dict: åŒ…å«æ‰€æœ‰é—œè¯é—œä¿‚çš„å­—å…¸
    """
    relationships = {
        'primary_chain': {},
        'auxiliary_chain': {},
        'geographic_chain': {},
        'relationship_strength': {}
    }
    
    try:
        # ä¸»éˆè·¯é—œè¯
        if len(df_scan) > 0 and len(df_light) > 0:
            # User â†’ Session â†’ Scan â†’ Light é—œè¯
            scan_light_merge = df_scan.merge(df_light, on='lig_id', how='left')
            relationships['primary_chain']['user_session_scan_light'] = scan_light_merge
            
        if len(df_light) > 0 and len(df_coor) > 0:
            # Light â†’ Coordinate â†’ Scene é—œè¯
            light_coor_merge = df_light.merge(df_coor, on='coor_id', how='left')
            relationships['primary_chain']['light_coordinate_scene'] = light_coor_merge
            
        if len(df_coor) > 0 and len(df_arobjs) > 0:
            # Scene â†’ AR_Object é—œè¯
            scene_obj_merge = df_coor.merge(df_arobjs, on='scene_id', how='left')
            relationships['primary_chain']['scene_ar_object'] = scene_obj_merge
            
        if len(df_arobjs) > 0 and len(df_click_lig) > 0:
            # AR_Object â†’ Click é—œè¯
            obj_click_merge = df_arobjs.merge(df_click_lig, on='obj_id', how='left')
            relationships['primary_chain']['ar_object_click'] = obj_click_merge
            
        # è¼”åŠ©éˆè·¯é—œè¯
        if len(df_pj_code) > 0 and len(df_click_lig) > 0:
            # Project â†’ Click_Analytics é—œè¯
            project_click_merge = df_click_lig.merge(df_pj_code, on='pj_code', how='left')
            relationships['auxiliary_chain']['project_click_analytics'] = project_click_merge
            
        # åœ°ç†éˆè·¯é—œè¯  
        if len(df_light) > 0 and len(df_field) > 0:
            # Coordinate â†’ City é—œè¯ï¼ˆåŸºæ–¼ç¶“ç·¯åº¦ï¼‰
            coordinate_city_merge = df_light.merge(df_field, left_on='field_id', right_on='field_id', how='left')
            relationships['geographic_chain']['coordinate_city'] = coordinate_city_merge
            
        # è¨ˆç®—é—œè¯å¼·åº¦
        relationships['relationship_strength'] = calculate_relationship_strength(relationships)
        
        print(f"æ•¸æ“šé—œè¯æ¨¡å‹å»ºç«‹å®Œæˆï¼ŒåŒ…å« {len(relationships)} å€‹é—œè¯éˆè·¯")
        return relationships
        
    except Exception as e:
        print(f"å»ºç«‹æ•¸æ“šé—œè¯æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return relationships


def calculate_relationship_strength(relationships):
    """
    è¨ˆç®—é—œè¯å¼·åº¦
    
    Args:
        relationships: é—œè¯é—œä¿‚å­—å…¸
        
    Returns:
        dict: é—œè¯å¼·åº¦æŒ‡æ¨™
    """
    strength_metrics = {}
    
    for chain_type, chain_data in relationships.items():
        if chain_type == 'relationship_strength':
            continue
            
        chain_strength = {}
        for relation_name, relation_data in chain_data.items():
            if hasattr(relation_data, 'shape'):
                # è¨ˆç®—æ•¸æ“šå®Œæ•´æ€§
                total_records = len(relation_data)
                non_null_percentage = (relation_data.notna().sum().sum() / 
                                     (total_records * len(relation_data.columns)) * 100) if total_records > 0 else 0
                
                chain_strength[relation_name] = {
                    'total_records': total_records,
                    'data_completeness': round(non_null_percentage, 2),
                    'strength_score': min(100, round(non_null_percentage * (total_records / 1000), 2))
                }
        
        strength_metrics[chain_type] = chain_strength
    
    return strength_metrics


def track_user_journey():
    """
    è¿½è¹¤ç”¨æˆ¶å®Œæ•´è¡Œç‚ºè·¯å¾‘
    
    åˆ†æç”¨æˆ¶å¾æƒæåˆ°é»æ“Šçš„å®Œæ•´æ—…ç¨‹
    
    Returns:
        pd.DataFrame: ç”¨æˆ¶æ—…ç¨‹æ•¸æ“š
    """
    print("é–‹å§‹è¿½è¹¤ç”¨æˆ¶è¡Œç‚ºæ—…ç¨‹...")
    
    try:
        # å»ºç«‹å®Œæ•´çš„ç”¨æˆ¶æ—…ç¨‹æ•¸æ“š
        user_journey = pd.DataFrame()
        
        if len(df_scan) > 0 and len(df_click_lig) > 0:
            # åŸºæ–¼ç”¨æˆ¶IDå’Œæ™‚é–“åºåˆ—å»ºç«‹æ—…ç¨‹
            scan_with_user = df_scan.copy()
            click_with_user = df_click_lig.copy()
            
            # æå–ç”¨æˆ¶IDï¼ˆå‡è¨­å¾client_idæˆ–codenameä¸­æå–ï¼‰
            if 'client_id' in scan_with_user.columns:
                scan_with_user['user_id'] = scan_with_user['client_id']
            
            if 'user_id' in click_with_user.columns:
                # åˆä½µæƒæå’Œé»æ“Šæ•¸æ“š
                user_journey = pd.merge(
                    scan_with_user,
                    click_with_user,
                    on='user_id',
                    how='outer',
                    suffixes=('_scan', '_click')
                )
                
                # è¨ˆç®—æ—…ç¨‹æŒ‡æ¨™
                if 'scantime' in user_journey.columns and 'clicktime' in user_journey.columns:
                    user_journey['journey_duration'] = (
                        user_journey['clicktime'] - user_journey['scantime']
                    ).dt.total_seconds() / 60  # åˆ†é˜
                    
                # åˆ†ææ—…ç¨‹éšæ®µ
                user_journey['journey_stage'] = user_journey.apply(
                    lambda row: classify_journey_stage(row), axis=1
                )
                
                # è¨ˆç®—è½‰æ›ç‡
                user_journey['conversion_rate'] = user_journey.groupby('user_id').apply(
                    lambda group: len(group[group['clicktime'].notna()]) / len(group) * 100
                ).reset_index(level=0, drop=True)
        
        print(f"ç”¨æˆ¶æ—…ç¨‹è¿½è¹¤å®Œæˆï¼Œå…±è¿½è¹¤ {len(user_journey)} æ¢è¨˜éŒ„")
        return user_journey
        
    except Exception as e:
        print(f"è¿½è¹¤ç”¨æˆ¶æ—…ç¨‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return pd.DataFrame()


def classify_journey_stage(row):
    """
    åˆ†é¡æ—…ç¨‹éšæ®µ
    
    Args:
        row: æ•¸æ“šè¡Œ
        
    Returns:
        str: æ—…ç¨‹éšæ®µ
    """
    if pd.notna(row.get('scantime')) and pd.isna(row.get('clicktime')):
        return 'scan_only'
    elif pd.isna(row.get('scantime')) and pd.notna(row.get('clicktime')):
        return 'direct_click'
    elif pd.notna(row.get('scantime')) and pd.notna(row.get('clicktime')):
        return 'complete_journey'
    else:
        return 'unknown'


def cross_dimensional_analysis():
    """
    å¯¦ç¾å¤šç¶­åº¦äº¤å‰åˆ†æ
    
    æ”¯æ´ä»¥ä¸‹åˆ†æï¼š
    - æ™‚é–“ Ã— åœ°ç†åˆ†æ
    - ç”¨æˆ¶ Ã— è¡Œç‚ºåˆ†æ  
    - å…§å®¹ Ã— æ•ˆæœåˆ†æ
    
    Returns:
        dict: å¤šç¶­åº¦åˆ†æçµæœ
    """
    print("é–‹å§‹å¤šç¶­åº¦äº¤å‰åˆ†æ...")
    
    analysis_results = {
        'time_geographic': {},
        'user_behavior': {},
        'content_effectiveness': {}
    }
    
    try:
        # æ™‚é–“ Ã— åœ°ç†åˆ†æ
        if len(df_scan) > 0 and len(df_light) > 0:
            time_geo_data = df_scan.merge(df_light, on='lig_id', how='left')
            
            if 'scantime' in time_geo_data.columns:
                # æŒ‰æ™‚æ®µå’Œåœ°ç†ä½ç½®åˆ†çµ„åˆ†æ
                time_geo_analysis = time_geo_data.groupby([
                    time_geo_data['scantime'].dt.hour,
                    'field_id'
                ]).agg({
                    'lig_id': 'count',
                    'session_duration': 'mean',
                    'device_type': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'unknown'
                }).rename(columns={'lig_id': 'scan_count'})
                
                analysis_results['time_geographic'] = time_geo_analysis
        
        # ç”¨æˆ¶ Ã— è¡Œç‚ºåˆ†æ
        if len(df_click_lig) > 0:
            user_behavior_analysis = df_click_lig.groupby('user_id').agg({
                'obj_id': 'count',
                'interaction_time': ['mean', 'sum'],
                'gesture_type': lambda x: x.value_counts().to_dict(),
                'attention_duration': 'mean'
            })
            
            analysis_results['user_behavior'] = user_behavior_analysis
        
        # å…§å®¹ Ã— æ•ˆæœåˆ†æ
        if len(df_arobjs) > 0 and len(df_click_lig) > 0:
            content_effectiveness = df_arobjs.merge(df_click_lig, on='obj_id', how='left')
            
            effectiveness_metrics = content_effectiveness.groupby('obj_name').agg({
                'clicktime': 'count',
                'interaction_time': 'mean',
                'attention_duration': 'mean',
                'gesture_type': lambda x: x.value_counts().to_dict()
            }).rename(columns={'clicktime': 'click_count'})
            
            # è¨ˆç®—æ•ˆæœè©•åˆ†
            effectiveness_metrics['effectiveness_score'] = (
                effectiveness_metrics['click_count'] * 0.4 +
                effectiveness_metrics['interaction_time'] * 0.3 +
                effectiveness_metrics['attention_duration'] * 0.3
            )
            
            analysis_results['content_effectiveness'] = effectiveness_metrics
        
        print("å¤šç¶­åº¦äº¤å‰åˆ†æå®Œæˆ")
        return analysis_results
        
    except Exception as e:
        print(f"å¤šç¶­åº¦äº¤å‰åˆ†ææ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return analysis_results


# %% æ¸¬è©¦
if __name__ == "__main__":
    print("æ¸¬è©¦")
